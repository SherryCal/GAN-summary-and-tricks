What is autoencoder does is it takes some kind of input data, which could be an image or a vector anything at all with a very high dimensionality, it is gonna to run it through this neural network and it is gonna to try to compress the data into a smaller representation. It does this with two principal components. The first components is what we call the encoder, the encoder is simply a bunch of layers they can be fully connected layers or convolutional they are going to take the input and then it is going to compress it down to a small representation which has less dimensions than the input and this is what we call the bottleneck, and then the bottleneck is going to try and reconstruct the input by using again fully connected or convolutional layers. And then the loss function of training an autoencoder is simply looking at the reconstructed version at the end of your encoder network. Then, you are going to simply compute the reconstruction with respect to your input and simply comparing pixel to pixel differences in the output. We can create a loss function and we can start training our network to compress images, and so obviously you have simple encoders that use fully connected layers but you can just as well swap them out with convolution
If you are looking what is going on here. if you train a deep convolutional network to do encoding and decoding of a whole brunch of images, you are actually creating a whole new kind of compression algorithm.
When we get reconstructions from autoencoders it look pretty ok but maybe fuzzy. The fuzziness is because you force the entire information of your image to go through the bottleneck layers dimensionalities. And then when we construct obviously lose some of that detail.
