# paper
Autoencoders (AE)[6] are neural networks that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. This kind of network is composed of two parts: encoder and decoder. Encoder is the part of the network that compresses the input into a latent-space representation. Decoder aims to reconstruct the input from the latent space representation. Using backpropagation, this unsupervised algorithm continuously trains itself by setting the target output values to equal the inputs. This forces the smaller hidden encoding layer to use dimensional reduction to eliminate noise and reconstruct the inputs. Autoencoders are learned automatically from data examples. It means that it is easy to train specialized instances of the algorithm that will perform well on a specific type of input and that it does not require any new engineering, only the appropriate training data.
![image](https://github.com/SherryCal/related-work-summary-and-tricks/blob/master/autoencoder/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202019-11-08%20%E4%B8%8B%E5%8D%882.48.02.png)
# Pre
What is autoencoder does is it takes some kind of input data, which could be an image or a vector anything at all with a very high dimensionality, it is gonna to run it through this neural network and it is gonna to try to compress the data into a smaller representation. It does this with two principal components. The first components is what we call the encoder, the encoder is simply a bunch of layers they can be fully connected layers or convolutional they are going to take the input and then it is going to compress it down to a small representation which has less dimensions than the input and this is what we call the bottleneck, and then the bottleneck is going to try and reconstruct the input by using again fully connected or convolutional layers. And then the loss function of training an autoencoder is simply looking at the reconstructed version at the end of your encoder network. Then, you are going to simply compute the reconstruction with respect to your input and simply comparing pixel to pixel differences in the output. We can create a loss function and we can start training our network to compress images, and so obviously you have simple encoders that use fully connected layers but you can just as well swap them out with convolution If you are looking what is going on here. if you train a deep convolutional network to do encoding and decoding of a whole brunch of images, you are actually creating a whole new kind of compression algorithm. When we get reconstructions from autoencoders it look pretty ok but maybe fuzzy. The fuzziness is because you force the entire information of your image to go through the bottleneck layers dimensionalities. And then when we construct obviously lose some of that detail.
![image](https://github.com/SherryCal/related-work-summary-and-tricks/blob/master/autoencoder/%E5%9B%BE%E7%89%87%201.png)
![image](https://github.com/SherryCal/related-work-summary-and-tricks/blob/master/autoencoder/%E5%9B%BE%E7%89%87%202.png)


